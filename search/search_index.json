{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"documentation/","title":"PUBLISH TO SNOWFLAKE FROM DELTA - STREAM","text":"<p>This service provides the ability to publish data from one or more source Delta tables into target Snowflake tables via stage using structured streaming.</p> <p>For full documentation visit databricks.com/what-is-structured-streaming.</p>"},{"location":"documentation/#pre-requisites","title":"Pre-Requisites","text":"<ul> <li>The source table must be available and setup as NRT streaming</li> <li>Only one transformation from a streaming source to target snowflake table is allowed</li> <li>Snowflake Stage, Target database, and schemas should exist</li> <li>The <code>update_ts_column</code> must be present in sql clause</li> </ul>"},{"location":"documentation/#configuration","title":"Configuration","text":"<pre><code>job_steps:\n    - job_step:\n        service_name: nrt_snowflake_from_delta\n        service_config:\n            environments:\n                nonprod:\n                    env_suffix: _nonprod\n                    sf_target_database: bi_dev\n                    sf_stage_database: source_dev\n                    trigger:\n                        once: true\n                preprod:\n                    env_suffix: _preprod\n                    sf_target_database: bi_preprod\n                    sf_stage_database: source_preprod\n                prod:\n                    env_suffix:\n                    sf_target_database: bi_prod_donothing\n                    sf_stage_database: source\n            transformation_step:\n                sf_stage_schema: stage\n                sf_target_schema: client_analytics\n                sf_target_table: entitlement2\n                primary_key: related_entity_id,related_entity_type,version,permission_type,permission_holder_type,\npermission_holder_id\n                exclude_cols: cur_ind\n                update_condition: source.cur_ind = true\n                delete_condition: source.cur_ind = false\n                insert_condition: source.cur_ind = true\n                source_database: refinery_event{env_suffix}\n                source_table: ges_entitlement\n                source_sql:\n                    select\n                        related_entity_id,\n                        related_entity_type,\n                        version,\n                        permission_type,\n                        permission_holder_type,\n                        permission_holder_id,\n                        cur_ind,\n                        insert_ts,\n                        update_ts\n                    from\n                        stream_source_table\n</code></pre> <p>Note</p> <p>The following parameter values are required but can be left empty: <code>update_condition</code> <code>delete_condition</code> <code>insert_condition</code></p> <p>Note</p> <p>The following parameter values are optional but cannot be left empty: <code>exlude_cols</code> <code>merge_sql</code> <code>partition_columns</code></p>"},{"location":"documentation/#schedule","title":"Schedule","text":"<p>For lenient latency requirements use the optional trigger parameter and set the expression <code>once: true</code>. The trigger option is situated in the environment to allow non-prod work streams to be scheduled intraday for cost saving.</p>"},{"location":"documentation/#full-publish-support","title":"Full Publish Support","text":"<pre><code>job_steps:\n    - job_step:\n        service_name: nrt_snowflake_from_delta\n        service_config:\n            environments:\n                nonprod:\n                    env_suffix: _nonprod\n                    sf_target_database: ods_dev\n                preprod:\n                    env_suffix: _preprod\n                    sf_target_database: ods_preprod\n                prod:\n                    env_suffix:\n                    sf_target_database: ods_production\n            transformation_step:\n                sf_target_schema: event\n                sf_target_table: genre\n                overwrite_target: true\n                source_sql:\n                    select\n                        genre_id,\n                        segment_id,\n                        genre_nm,\n                        insert_ts,\n                        update_ts,\n                        last_update_dttm\n                    from\n                        default.discovery_genre\n</code></pre> <p>Note</p> <ol> <li><code>update_ts</code> column will be used for deduping</li> <li>The sql statement should use/reference the temporary table <code>stream_source_table</code></li> <li>Parameter values for <code>update_condition</code>, <code>delete_condition</code>, <code>insert_condition</code> should use alias target source to create the filter <code>expression: target</code>. Example: <code>&lt;column_name&gt;&lt;any_comparision_operators&gt;&lt;source_column_name&gt; | &lt;literal value&gt;</code></li> <li>Use full publish support for once a day whole table refresh requirement</li> </ol>"},{"location":"documentation/#service-configuration","title":"Service Configuration","text":"<ol> <li>Clone to you home folder the development/testing notebook: <code>workspace/cds_nonprod/stream_snowflake_from_Delta_Develop</code></li> <li>Update the configuration to meet the requirement</li> <li>Test the configuration</li> <li>QA target table</li> <li>Checkin the configuration to GIT</li> <li>Merge the changes to develop the branch</li> </ol>"}]}